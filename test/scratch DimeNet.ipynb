{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sympy as sym\n",
    "import math\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = torch.from_numpy(np.random.rand(5, 10)).to(torch.float32)\n",
    "node_pos = torch.from_numpy(np.random.rand(5, 3)).to(torch.float32)\n",
    "edge_index = np.array([[0, 1], [1, 0],\n",
    "                        [0, 2], [2, 0],\n",
    "                        [0, 3], [3, 0],\n",
    "                        [0, 4], [4, 0],\n",
    "                        [1, 2], [2, 1],\n",
    "                        [1, 4], [4, 1],\n",
    "                        [2, 3], [3, 2],\n",
    "                        [2, 4], [4, 2], \n",
    "                        [3, 4], [4, 3],]).T\n",
    "edge_index = torch.from_numpy(edge_index).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_number = torch.randint(0, 95, (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = torch.sum((node_pos[target] - node_pos[source]) ** 2, dim = -1) ** 0.5\n",
    "distance = distance.view(edge_index.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_edge_index_to_adjacency_list(edge_index: Tensor or np.array, num_nodes: int):\n",
    "    source, target = edge_index\n",
    "    adjacency_list = [[] for _ in range(num_nodes)]\n",
    "\n",
    "    for edge_id, (neighbor, vertex) in enumerate(zip(source, target)):\n",
    "        vertex, neighbor = int(vertex), int(neighbor)\n",
    "        adjacency_list[vertex].append((neighbor, edge_id))\n",
    "\n",
    "    return adjacency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(node_pos, edge_index, edge_1, edge_2):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_index_from_adjacency_list(adjacency_list: list, num_nodes: int) -> list:\n",
    "    r\"\"\"\n",
    "    Returns angle (a(kj, ji) and kj's edge index)\n",
    "    i<-j<-k (j is i's neighbor and k is j's neighbor)\n",
    "    \"\"\"\n",
    "    angle_index = []\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j, ji_edge_index in adjacency_list[i]:\n",
    "            for k, kj_edge_index in adjacency_list[j]:\n",
    "                if k == i:\n",
    "                    continue\n",
    "                \n",
    "                angle_index.append([kj_edge_index, ji_edge_index])\n",
    "\n",
    "    return angle_index   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_index = angle_index_from_adjacency_list(convert_edge_index_to_adjacency_list(edge_index, 5), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_index = torch.from_numpy(np.array(angle_index).T).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = torch.from_numpy(np.random.rand(angle_index.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Envelope(nn.Module):\n",
    "    def __init__(self, exponent: int):\n",
    "        super(Envelope, self).__init__()\n",
    "\n",
    "        self.p = exponent + 1\n",
    "        self.a = -(self.p + 1) * (self.p + 2) / 2\n",
    "        self.b = self.p * (self.p + 2)\n",
    "        self.c = - self.p * (self.p + 1) / 2\n",
    "\n",
    "    def forward(self, d: Tensor) -> Tensor:\n",
    "        exponent = self.p - 1\n",
    "        d_p = d.pow(exponent)\n",
    "        d_p_1 = d_p * d\n",
    "        d_p_2 = d_p_1 * d\n",
    "\n",
    "        return (1 / d + self.a * d_p + self.b * d_p_1 + self.c * d_p_2) * (d < 1.0).to(d.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBesselBasis(nn.Module):\n",
    "    def __init__(self, num_radial_basis: int, cut_off: float, envelope_exponent: int):\n",
    "        super(RadialBesselBasis, self).__init__()\n",
    "        self.num_radial_basis = num_radial_basis\n",
    "        self.cut_off = cut_off\n",
    "        self.envelope_function = Envelope(envelope_exponent)\n",
    "        \n",
    "        # shape [num_radial_basis]\n",
    "        self.wave_numbers = nn.Parameter(torch.Tensor(num_radial_basis))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"\n",
    "        Initialize wave numbers to n * pi\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            torch.arange(1, self.num_radial_basis + 1, out = self.wave_numbers).mul_(torch.pi)\n",
    "        self.wave_numbers.requires_grad_()\n",
    "\n",
    "    def forward(self, distance: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Compute Radial Basis Function representation of interatomic distance\n",
    "\n",
    "        Parameters:\n",
    "            distance (torch.Tensor):\n",
    "                Interatomic distance. Shape [E]\n",
    "\n",
    "        Returns:\n",
    "            distance_representation (torch.Tensor):\n",
    "                Shape [E, num_radial_basis]\n",
    "                \n",
    "        \"\"\"\n",
    "        distance_scaled = distance / self.cut_off # d / c\n",
    "        distance_scaled = distance_scaled.unsqueeze(-1)\n",
    "        # shape [E, num_radial_basis]\n",
    "        distance_representation = self.envelope_function(distance_scaled) * torch.sin(distance_scaled * self.wave_numbers)\n",
    "\n",
    "        return distance_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dimenet_utils import bessel_basis, real_sph_harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalBesselBasis(nn.Module):\n",
    "    def __init__(self, num_radial_basis: int, num_spherical_basis: int, cut_off: float, envelope_exponent: int):\n",
    "        super(SphericalBesselBasis, self).__init__()\n",
    "\n",
    "        self.num_radial_basis = num_radial_basis\n",
    "        self.num_spherical_basis = num_spherical_basis\n",
    "\n",
    "        self.cut_off = cut_off\n",
    "        self.envelope_function = Envelope(envelope_exponent)\n",
    "\n",
    "        bessel_formulas = bessel_basis(num_spherical_basis, num_radial_basis)\n",
    "        spherical_harmonics_formulas = real_sph_harm(num_spherical_basis)\n",
    "\n",
    "        self.bessel_functions = []\n",
    "        self.spherical_harmonics = []\n",
    "\n",
    "        # distance d & angle alpha\n",
    "        d, alpha = sym.symbols(\"x theta\")\n",
    "        modules = {'sin': torch.sin, 'cos': torch.cos}\n",
    "\n",
    "        for l in range(num_spherical_basis):\n",
    "            \n",
    "            if l == 0:\n",
    "                first_y = sym.lambdify([alpha], spherical_harmonics_formulas[l][0], modules)(0)\n",
    "                self.spherical_harmonics.append(lambda d: torch.zeros_like(d) + first_y)\n",
    "            else:\n",
    "                y = sym.lambdify([alpha], spherical_harmonics_formulas[l][0], modules)\n",
    "                self.spherical_harmonics.append(y)\n",
    "            \n",
    "            for n in range(num_radial_basis):\n",
    "                j = sym.lambdify([d], bessel_formulas[l][n], modules)\n",
    "                self.bessel_functions.append(j)\n",
    "\n",
    "    def forward(self, distance: Tensor, angle: Tensor, angle_index: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Compute angle representation using spherical Bessel functions and spherical harmonics\n",
    "\n",
    "        Parameters:\n",
    "            distance (torch.Tensor):\n",
    "                Interatomic distance. Shape [E]\n",
    "            angle (torch.Tensor):\n",
    "                Angle between 2 bonds. Shape [A] (A = number of angles)\n",
    "            angle_index (torch.Tensor):\n",
    "                Shape [2, A]\n",
    "        Returns: \n",
    "            angle_representation (torch.Tensor):\n",
    "                Shape [A, num_spherical_basis, num_radial_basis]\n",
    "        \"\"\"\n",
    "        kj_index, ji_index = angle_index\n",
    "        distance_scaled = distance / self.cut_off\n",
    "        \n",
    "        # shape [A, num_spherical_basis]\n",
    "        cbf = torch.stack([y(angle) for y in self.spherical_harmonics], dim = -1)\n",
    "\n",
    "        d_kj = distance_scaled[kj_index]\n",
    "        # shape [A, num_spherical_basis * num_radial_basis]\n",
    "        rbf = self.envelope_function(d_kj).unsqueeze(-1) * torch.stack([j(d_kj) for j in self.bessel_functions], dim = -1)\n",
    "        # shape [A, num_spherical_basis * num_radial_basis] -> [A, num_spherical_basis, num_radial_basis]\n",
    "        rbf = rbf.view(-1, self.num_spherical_basis, self.num_radial_basis)\n",
    "\n",
    "        # shape [A, num_spherical_basis, num_radial_basis]\n",
    "        angle_representation = rbf * cbf.unsqueeze(-1)\n",
    "        # shape [A, num_spherical_basis, num_radial_basls] -> [A, num_spherical_basis * num_radial_basis]\n",
    "        angle_representation = angle_representation.view(-1, self.num_spherical_basis * self.num_radial_basis)\n",
    "\n",
    "        return angle_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SphericalBesselBasis(2, 4, 0.5, 10) # 2 radial, 4 spherical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RadialBesselBasis(2, 0.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_representation = r(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_representation = s.forward(distance, angle, angle_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_representation = angle_representation.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(self, num_radial_basis: int, hidden_dim: int, activation = None):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "\n",
    "        self.atomic_num_embedding = nn.Embedding(95, hidden_dim)\n",
    "        self.linear_distance = nn.Linear(num_radial_basis, hidden_dim)\n",
    "        self.linear = nn.Linear(3 * hidden_dim, hidden_dim)\n",
    "        self.activatoin = activation\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.atomic_num_embedding.weight, -math.sqrt(3), math.sqrt(3))\n",
    "        nn.init.orthogonal_(self.linear_distance.weight)\n",
    "        nn.init.orthogonal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, atomic_number: Tensor, distance_representation: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Compute message embeddings \n",
    "\n",
    "        Parameters:\n",
    "            atomic_number (torch.Tensor):\n",
    "                Atoms' atomic number. Shape [N]\n",
    "            distance_representation (torch.Tensor):\n",
    "                Radial Basis function representation of interatomic distance. Shape [E, num_radial]\n",
    "            edge_index (torch.tensor):\n",
    "                Shape [2, E]\n",
    "                        \n",
    "        Returns:\n",
    "            message (torch.Tensor):\n",
    "                Message embeddings. Shape [E, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Shape [E, num_radial] -> [E, hidden_dim]\n",
    "        distance_representation = self.linear_distance(distance_representation)\n",
    "        \n",
    "        source, target = edge_index\n",
    "        \n",
    "        # Shape [E, hidden_dim]\n",
    "        h_i = self.atomic_num_embedding(atomic_number[target])\n",
    "        h_j = self.atomic_num_embedding(atomic_number[source])\n",
    "\n",
    "        # Shape [E, 3 * hidden_dim]\n",
    "        message_ji = torch.cat([h_j, h_i, distance_representation], dim = -1)   \n",
    "        # Shape [E, 3 * hidden_dim] -> [E, hidden_dim]\n",
    "        message = self.linear(message_ji)\n",
    "        if self.activatoin is not None:\n",
    "            message = self.activatoin(message)\n",
    "\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = EmbeddingBlock(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = e.forward(atomic_number, distance_representation, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, in_dim: int, use_bias = True, activation = None):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_dim, in_dim, bias = use_bias)\n",
    "        self.linear_2 = nn.Linear(in_dim, in_dim, bias = use_bias)\n",
    "        self.activation = activation\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.linear_1.weight)\n",
    "        if self.linear_1.bias is not None:\n",
    "            self.linear_1.bias.data.fill_(0)\n",
    "\n",
    "        nn.init.orthogonal_(self.linear_2.weight)\n",
    "        if self.linear_2.bias is not None:\n",
    "            self.linear_2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x_0 = x\n",
    "        \n",
    "        x = self.linear_1(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = self.linear_2(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "\n",
    "        return x_0 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, num_radial_basis: int, hidden_dim: int, out_dim: int, num_linear_layers: int, activation = None):\n",
    "        super(OutputBlock, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear_distance = nn.Linear(num_radial_basis, hidden_dim, bias = False)\n",
    "        self.linear_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, hidden_dim, bias = True) for _ in range(num_linear_layers)])\n",
    "        self.linear_out = nn.Linear(hidden_dim, out_dim, bias = False)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.linear_distance.weight)\n",
    "        \n",
    "        for linear_layer in self.linear_layers:\n",
    "            nn.init.orthogonal_(linear_layer.weight)\n",
    "            linear_layer.bias.data.fill_(0)\n",
    "\n",
    "        nn.init.orthogonal_(self.linear_out.weight)\n",
    "\n",
    "    def forward(self, distance_representation: Tensor, message: Tensor, edge_index: Tensor, num_nodes: int):\n",
    "        r\"\"\"\n",
    "        Transform message embeddings with distance representation and compute atom-wise output\n",
    "\n",
    "        Parameters: \n",
    "            distance_representation (torch.Tensor):\n",
    "                Radial Basis function representation of interatomic distance. Shape [E, num_radial_basis]\n",
    "            message (torch.Tensor):\n",
    "                Message embeddings. Shape [E, hidden_dim] \n",
    "            edge_index (torch.tensor):\n",
    "                Shape [2, E]\n",
    "            num_nodes (int)\n",
    "\n",
    "        Returns: \n",
    "            out_node_embedding (torch.Tensor):\n",
    "                Shape [N, out_dim]\n",
    "        \"\"\"\n",
    "        # shape [E, num_radial_basis] -> [E, hidden_dim]\n",
    "        distance_representation = self.linear_distance(distance_representation)\n",
    "        # shape [E, hidden_dim]\n",
    "        transformed_message = distance_representation * message\n",
    "\n",
    "        # aggregrate message for each atom\n",
    "        source_index, target_index = edge_index\n",
    "        # shape [E] -> [E, hidden_dim]\n",
    "        target_index_lifted = torch.broadcast_to(target_index.unsqueeze(-1), (edge_index.shape[-1], self.hidden_dim))\n",
    "        # shape [N, hidden_dim]\n",
    "        node_embedding = torch.zeros(num_nodes, self.hidden_dim).scatter_add_(0, target_index_lifted, transformed_message)\n",
    "\n",
    "        for linear_layer in self.linear_layers:\n",
    "            node_embedding = linear_layer(node_embedding)\n",
    "            if self.activation is not None:\n",
    "                node_embedding = self.activation(node_embedding)\n",
    "\n",
    "        # shape [N, hidden_dim] -> [N, out_dim]\n",
    "        out_node_embedding = self.linear_out(node_embedding)\n",
    "\n",
    "        return out_node_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_block = OutputBlock(2, 10, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0083],\n",
       "        [ 0.0000],\n",
       "        [-1.3686],\n",
       "        [-0.0014],\n",
       "        [-1.0226]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_block.forward(distance_representation, message, edge_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, num_radial_basis: int, num_spherical_basis: int, hidden_dim: int, bilinear_dim: int, \n",
    "                message_in_dim: int, num_layers_before_skip: int, num_layers_after_skip: int, activation = None):\n",
    "        super(InteractionBlock, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        self.linear_distance = nn.Linear(num_radial_basis, hidden_dim, bias = False)\n",
    "        self.linear_angle = nn.Linear(num_spherical_basis * num_radial_basis, bilinear_dim, bias = False)\n",
    "\n",
    "        self.linear_source_message = nn.Linear(message_in_dim, hidden_dim) # kj\n",
    "        self.linear_target_message = nn.Linear(message_in_dim, hidden_dim) # ji\n",
    "\n",
    "        self.weight_bilinear = nn.Parameter(torch.Tensor(hidden_dim, bilinear_dim, hidden_dim))\n",
    "\n",
    "        self.layers_before_skip = nn.ModuleList([\n",
    "            ResidualLayer(hidden_dim, use_bias = True, activation = activation) for _ in range(num_layers_before_skip)\n",
    "        ])\n",
    "\n",
    "        self.linear_skip = nn.Linear(hidden_dim, message_in_dim)\n",
    "\n",
    "        self.layers_after_skip = nn.ModuleList([\n",
    "            ResidualLayer(message_in_dim, use_bias = True, activation = activation) for _ in range(num_layers_after_skip)\n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.linear_distance.weight)\n",
    "        nn.init.orthogonal_(self.linear_angle.weight)\n",
    "        \n",
    "        nn.init.orthogonal_(self.linear_source_message.weight)\n",
    "        self.linear_source_message.bias.data.fill_(0)\n",
    "        nn.init.orthogonal_(self.linear_target_message.weight)\n",
    "        self.linear_target_message.bias.data.fill_(0)\n",
    "\n",
    "        self.weight_bilinear.data.normal_(mean = 0, std = 2 / self.hidden_dim)\n",
    "\n",
    "        for layer in self.layers_before_skip:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "        nn.init.orthogonal_(self.linear_skip.weight)\n",
    "        self.linear_skip.bias.data.fill_(0)\n",
    "\n",
    "        for layer in self.layers_before_skip:\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    def forward(self, distance_representation: Tensor, angle_representation: Tensor, message: Tensor, \n",
    "                angle_index: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Update message embeddings\n",
    "\n",
    "        Parameters: \n",
    "            distance_representation (torch.Tensor):\n",
    "                Shape [E, num_radial_basis]\n",
    "            angle_representation (torch.Tensor):\n",
    "                Shape [E, num_shperical_basis, num_radial_basis]\n",
    "            message (torch.Tensor):\n",
    "                Shape [E, message_in_dim]\n",
    "            angle_index (torch.tensor):\n",
    "                Shape [2, A]\n",
    "            \n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        num_edges, num_angles = distance_representation.shape[0], angle_index.shape[-1]\n",
    "        # shape [E, num_radial_basis] -> [E, hidden_dim]\n",
    "        distance_representation = self.linear_distance(distance_representation)\n",
    "        # shape [A, num_spherical_basis * num_radial_basis] -> [A, bilinear_dim]\n",
    "        angle_representation = self.linear_angle(angle_representation)\n",
    "\n",
    "        source_edge, target_edge = angle_index\n",
    "        source_message, target_message = message[source_edge], message[target_edge]\n",
    "        # shape [A, message_in_dim] -> [A, hidden_dim]\n",
    "        source_message = self.linear_source_message(source_message)\n",
    "        source_message = source_message * distance_representation[target_edge]\n",
    "        # shape [A, hidden_dim]\n",
    "        source_message = torch.einsum('ab,ah,ibh->ai', angle_representation, source_message, self.weight_bilinear)\n",
    "\n",
    "        # aggregrate source message\n",
    "        # shape [A, hidden_dim]\n",
    "        target_index_lifted = torch.broadcast_to(target_edge.unsqueeze(-1), (num_angles, self.hidden_dim))\n",
    "        # shape [E, hidden_dim]\n",
    "        aggregrated_message = torch.zeros(num_edges, self.hidden_dim).scatter_add_(0, target_index_lifted, source_message) \n",
    "        \n",
    "        # residual\n",
    "        x, x_0 = aggregrated_message + self.linear_target_message(message), message\n",
    "\n",
    "        for layer in self.layers_before_skip:\n",
    "            x = layer(x)\n",
    "                \n",
    "        # shape [E, hidden_dim] -> [E, message_in_dim]\n",
    "        x = self.linear_skip(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = x + x_0\n",
    "        # shape [E, message_in_dim] -> [E, message_in_dim]\n",
    "        for layer in self.layers_after_skip:\n",
    "            x = layer(x)\n",
    "\n",
    "        updated_message = x\n",
    "        \n",
    "        return updated_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = InteractionBlock(2, 4, 10, 20, 10, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.3745e-02,  1.1036e+00, -2.0511e+00, -1.3143e+00, -4.4056e-01,\n",
       "         -9.0525e-01,  8.2946e-02, -2.9313e-02,  1.5946e+00, -8.6594e-01],\n",
       "        [ 8.9944e-02, -8.6496e-01,  3.4277e+00,  1.2736e+00, -1.7344e+00,\n",
       "          1.4645e+00,  3.9150e+00,  2.2253e+00, -1.9218e-01,  3.6730e+00],\n",
       "        [-9.4938e-01,  8.5199e-01, -1.8713e+00, -8.5834e-01, -9.5233e-01,\n",
       "         -1.8148e+00,  1.0485e-01, -4.1682e-01,  2.2274e+00, -2.4309e+00],\n",
       "        [ 1.1921e+00,  4.5081e-01,  2.7598e+00,  7.8600e-01, -1.3935e+00,\n",
       "         -6.8036e-01,  4.1401e+00,  1.4762e+00,  2.7835e-01, -3.3624e+00],\n",
       "        [ 4.8205e-01,  4.1498e-01, -1.1483e+00, -1.5955e+00,  4.4969e-01,\n",
       "         -2.5749e-01,  5.8162e-01, -1.1510e+00,  1.2045e+00, -1.3637e+00],\n",
       "        [ 1.4362e+00,  5.4642e-01,  2.1662e+00,  9.0338e-01, -1.9436e-01,\n",
       "          3.2279e-01,  3.8620e+00,  1.5209e+00, -1.0052e+00,  8.9920e-01],\n",
       "        [ 2.3833e-01,  1.8786e+00, -3.2117e+00, -1.3342e+00, -1.4409e+00,\n",
       "         -2.5367e+00,  1.7553e-02, -1.2353e+00,  1.9822e+00, -3.2945e+00],\n",
       "        [-1.2868e-01,  5.4381e-01,  2.6299e+00,  1.9627e+00, -2.2221e+00,\n",
       "          2.0532e-01,  4.6787e+00,  2.2460e+00,  9.6722e-01,  3.4344e-01],\n",
       "        [-2.0099e+00, -1.4516e+00,  1.7935e+00,  1.3962e+00, -1.3539e+00,\n",
       "          2.1326e+00,  1.5075e-01,  4.1179e+00,  3.4682e-01,  4.2112e+00],\n",
       "        [-2.0837e-03,  1.1578e-01,  9.4571e-01,  4.5255e-01, -5.0125e-01,\n",
       "          8.9726e-01,  3.5397e-01,  3.7563e+00,  1.8451e-01, -1.2592e+00],\n",
       "        [-8.2068e-01, -4.2426e-01,  4.5300e-01,  9.2006e-01, -1.8442e+00,\n",
       "          1.4097e+00,  6.4008e-02,  3.2998e+00,  1.0216e-01,  3.3478e+00],\n",
       "        [-1.3219e+00,  2.0799e-01,  8.1419e-01,  1.6240e+00, -1.3333e+00,\n",
       "          1.7815e+00,  8.9185e-01,  4.5231e+00,  8.7449e-01,  2.4472e+00],\n",
       "        [ 5.3696e-01, -5.6719e-01,  1.8477e+00,  1.6885e-01,  3.7329e-01,\n",
       "          1.5363e+00,  8.5767e-01,  2.6391e+00, -2.0034e-01, -1.7561e+00],\n",
       "        [-6.5040e-01, -3.4606e-02,  5.3107e-01,  1.0234e+00,  1.7041e-01,\n",
       "          9.8216e-01,  1.0283e-01,  3.4179e+00, -4.6093e-01,  1.4384e+00],\n",
       "        [-6.6559e-01,  3.5156e-01, -3.2553e-01,  2.4791e-01, -4.9762e-01,\n",
       "         -7.5021e-01, -9.4163e-02,  1.8072e+00,  4.7018e-01, -4.6667e+00],\n",
       "        [-3.1740e+00, -5.8469e-01,  8.8504e-01,  1.8955e+00, -8.3881e-01,\n",
       "          8.5627e-01,  5.3113e-01,  3.3923e+00,  1.4042e+00, -9.6417e-02],\n",
       "        [ 5.3880e-01,  9.9268e-01, -8.0942e-01,  5.4731e-01, -3.1989e-01,\n",
       "          2.5926e-01,  1.6092e-02,  2.5999e+00, -7.0559e-01,  5.7489e-01],\n",
       "        [-7.8284e-01, -4.7498e-01,  1.7162e+00,  1.3403e+00, -4.5877e-01,\n",
       "          2.4205e+00,  1.3955e+00,  3.4059e+00,  4.8963e-01,  1.9503e+00]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.forward(distance_representation, angle_representation.to(torch.float32), message, angle_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimeNet(nn.Module):\n",
    "    def __init__(self, num_radial_basis: int, num_spherical_basis: int, embedding_dim: int, bilinear_dim: int,\n",
    "            out_dim: int, cut_off: float, envelope_exponent: int, num_interaction_blocks: int, \n",
    "            num_layers_before_skip: int, num_layers_after_skip: int, num_output_linear_layers: int,\n",
    "            activation = None):\n",
    "        super(DimeNet, self).__init__()\n",
    "\n",
    "        self.rbf = RadialBesselBasis(num_radial_basis, cut_off, envelope_exponent)\n",
    "        self.sbf = SphericalBesselBasis(num_radial_basis, num_spherical_basis, cut_off, envelope_exponent)\n",
    "\n",
    "        self.embedding_block = EmbeddingBlock(num_radial_basis, embedding_dim, activation)\n",
    "\n",
    "        self.interaction_blocks = nn.ModuleList([\n",
    "            InteractionBlock(num_radial_basis, num_spherical_basis, embedding_dim, bilinear_dim, \n",
    "            embedding_dim, num_layers_before_skip, num_layers_after_skip, \n",
    "            activation) for _ in range(num_interaction_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([\n",
    "            OutputBlock(num_radial_basis, embedding_dim, out_dim, \n",
    "                num_output_linear_layers, activation) for _ in range(num_interaction_blocks + 1) \n",
    "        ])\n",
    "\n",
    "        self.reset_parameters()        \n",
    "\n",
    "    def reset_parameters(self):        \n",
    "        self.rbf.reset_parameters()\n",
    "        self.embedding_block.reset_parameters()\n",
    "\n",
    "        for interaction_block in self.interaction_blocks:\n",
    "            interaction_block.reset_parameters()\n",
    "\n",
    "        for output_block in self.output_blocks:\n",
    "            output_block.reset_parameters()\n",
    "\n",
    "    def forward(self, atomic_number: Tensor, edge_index: Tensor, angle_index: Tensor, distance: Tensor, \n",
    "        angle: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Parameters:\n",
    "            atomic_number (torch.Tensor):\n",
    "                Atomic number of atoms. Shape [N]\n",
    "            edge_index (torch.tensor):\n",
    "                Shape [2, E]\n",
    "            angle_index (torch.tensor):\n",
    "                Shape [2, A]\n",
    "            distance (torch.Tensor):\n",
    "                Shape [E]\n",
    "            angle (torch.Tensor):\n",
    "                Shape [A]\n",
    "        \n",
    "        Returns:\n",
    "            output (torch.Tensor)\n",
    "        \"\"\"\n",
    "\n",
    "        num_nodes = atomic_number.shape[0]\n",
    "\n",
    "        distance_representation = self.rbf(distance)\n",
    "        angle_representation = self.sbf(distance, angle, angle_index).to(torch.float32)\n",
    "\n",
    "        message = self.embedding_block(atomic_number, distance_representation, edge_index)\n",
    "        t = self.output_blocks[0](distance_representation, message, edge_index, num_nodes)\n",
    "\n",
    "        for interaction_block, output_block in zip(self.interaction_blocks, self.output_blocks[1:]):\n",
    "            message = interaction_block.forward(distance_representation, angle_representation, message,\n",
    "                    angle_index)\n",
    "            t = t + output_block(distance_representation, message, edge_index, num_nodes)\n",
    "\n",
    "        output = torch.sum(t, dim = 0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DimeNet(4, 2, 10, 20, 1, 0.5, 10, 3, 2, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.9700], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.forward(atomic_number, edge_index, angle_index, distance, angle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (v3.8.10:3d8993a744, May  3 2021, 09:09:08) \n[Clang 12.0.5 (clang-1205.0.22.9)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

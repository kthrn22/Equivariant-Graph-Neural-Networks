{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import Tensor, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = torch.from_numpy(np.random.rand(5, 10)).to(torch.float32)\n",
    "node_pos = torch.from_numpy(np.random.rand(5, 3)).to(torch.float32)\n",
    "edge_index = np.array([[0, 1], [1, 0],\n",
    "                        [0, 2], [2, 0],\n",
    "                        [0, 3], [3, 0],\n",
    "                        [0, 4], [4, 0],\n",
    "                        [1, 2], [2, 1],\n",
    "                        [1, 4], [4, 1],\n",
    "                        [2, 3], [3, 2],\n",
    "                        [2, 4], [4, 2], \n",
    "                        [3, 4], [4, 3],]).T\n",
    "edge_index = torch.from_numpy(edge_index).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_feat = torch.from_numpy(np.random.rand(5, 10, 3)).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_number = torch.randint(0, 95, (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "        self.atomic_num_embedding = nn.Embedding(95, embedding_dim, padding_idx = 0)\n",
    "\n",
    "    def forward(self, atomic_num) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Initialize \n",
    "\n",
    "        Parameters:\n",
    "            atomic_num (torch.tensor):\n",
    "                Shape [N]\n",
    "\n",
    "        Returns:\n",
    "            scalar_feat (torch.Tensor):\n",
    "                Shape [N, embedding_dim]\n",
    "        \"\"\"\n",
    "        scalar_feat = self.atomic_num_embedding(atomic_num)\n",
    "        return scalar_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledSiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledSiLU, self).__init__()\n",
    "        self.scale_factor = 1 / 0.6\n",
    "        self.activation = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x) * self.scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFunction(nn.Module):\n",
    "    def __init__(self, num_radial_basis: int, cut_off: float, trainable: bool = False):\n",
    "        super(RadialBasisFunction, self).__init__()\n",
    "        self.num_radial_basis = num_radial_basis\n",
    "        self.cut_off = cut_off\n",
    "\n",
    "        expanded_distance = nn.Parameter(torch.Tensor(num_radial_basis))\n",
    "        with torch.no_grad():\n",
    "            torch.arange(1, num_radial_basis + 1, out = expanded_distance).mul_(torch.pi)\n",
    "        \n",
    "        if trainable:\n",
    "            expanded_distance.requires_grad_()\n",
    "        else:\n",
    "            self.register_buffer(\"expanded_distance\", expanded_distance)\n",
    "\n",
    "        self.expanded_distance = expanded_distance\n",
    "\n",
    "    def forward(self, distance: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Construct radial basis for distance\n",
    "\n",
    "        Parameters:\n",
    "            distance (torch.Tensor):\n",
    "                Interatomic distance. Shape [E]\n",
    "\n",
    "        Returns:\n",
    "            expanded_distance (torch.Tensor):\n",
    "                Shape [E, num_radial_basis]\n",
    "        \"\"\"\n",
    "        distance_scaled = distance / self.cut_off\n",
    "\n",
    "        # shape [E, num_radial_basis]\n",
    "        expanded_distance = torch.sin(self.expanded_distance * distance_scaled.unsqueeze(-1)) / distance.unsqueeze(-1)\n",
    "\n",
    "        return expanded_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineCutoff(nn.Module):\n",
    "    def __init__(self, cut_off: float):\n",
    "        super(CosineCutoff, self).__init__()\n",
    "        self.cut_off = cut_off\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * (1 + torch.cos(x * torch.pi / self.cut_off)) * (x < self.cut_off).to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_radial_basis, cut_off):\n",
    "        super(MessageBlock, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.scalar_feat_proj = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            ScaledSiLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim * 3)   \n",
    "        )\n",
    "        self.radial_basis = RadialBasisFunction(num_radial_basis, cut_off)\n",
    "        self.rbf_proj = nn.Linear(num_radial_basis, embedding_dim * 3)\n",
    "        self.cosine_cut_off = CosineCutoff(cut_off)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.scalar_feat_proj[0].weight)\n",
    "        self.scalar_feat_proj[0].bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.scalar_feat_proj[2].weight)\n",
    "        self.scalar_feat_proj[2].bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.rbf_proj.weight)\n",
    "        self.rbf_proj.bias.data.fill_(0)        \n",
    "\n",
    "    def forward(self, vectorial_feat: Tensor, scalar_feat: Tensor, node_pos: Tensor, edge_index: Tensor):\n",
    "        r\"\"\"\n",
    "        Parameters:\n",
    "            vectorial_feat (torch.Tensor):\n",
    "                Vectorial representations. Shape [N, embedding_dim, 3]\n",
    "            scalar_feat (torch.Tensor):\n",
    "                Scalar representations. Shape [N, embedding_dim]\n",
    "            edge_index (torch.Tensor):\n",
    "                Shape [2, E]\n",
    "            node_pos (torch.Tensor):\n",
    "                Atom's 3D coordinates. Shape [N, 3]\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        num_nodes, num_edges = node_pos.shape[0], edge_index.shape[-1]\n",
    "        # shape [N, embedding_dim] -> [N, embedding_dim * 3]\n",
    "        scalar_feat = self.scalar_feat_proj(scalar_feat)\n",
    "\n",
    "        source, target = edge_index\n",
    "        # r_ij = r_i - r_j. Shape [E, 3]\n",
    "        relative_distance = node_pos[target] - node_pos[source]\n",
    "        # Shape [E]\n",
    "        distance = torch.sum(relative_distance ** 2, dim = -1) ** 0.5\n",
    "        # Shape [E, num_radial_basis]\n",
    "        expanded_distance = self.radial_basis(distance)\n",
    "        # Shape [E, num_radial_basis] -> [E, embedding_dim * 3]\n",
    "        filter = self.rbf_proj(expanded_distance)\n",
    "        filter = self.cosine_cut_off(filter)\n",
    "\n",
    "        # Shape [E, embedding_dim * 3]\n",
    "        message = scalar_feat[source] * filter\n",
    "        # Shape [E, embedding_dim * 3] -> [E, embedding_dim, 3]\n",
    "        message = message.view(-1, self.embedding_dim, 3)#.permute(2, 0, 1)\n",
    "        # Shape [E, embedding_dim, 1]\n",
    "        scalar_message, equivariant_vectorial_message, invariant_vectorial_message = torch.split(message, [1, 1, 1], dim = -1)\n",
    "        \n",
    "        # Shape [E, embedding_dim]\n",
    "        scalar_message = scalar_message.squeeze(-1)\n",
    "        equivariant_vectorial_message = equivariant_vectorial_message.squeeze(-1)\n",
    "        invariant_vectorial_message = invariant_vectorial_message.squeeze(-1)\n",
    "\n",
    "        # aggregrate message\n",
    "        target_index_lifted = torch.broadcast_to(target.unsqueeze(-1), (num_edges, self.embedding_dim))\n",
    "\n",
    "        # shape [N, embedding_dim]\n",
    "        scalar_message = torch.zeros(num_nodes, self.embedding_dim).scatter_add_(0, target_index_lifted, scalar_message)\n",
    "\n",
    "        # shape [E, 3] -> [E, embedding_dim, 3]\n",
    "        relative_distance = torch.broadcast_to((relative_distance / distance.unsqueeze(-1)).unsqueeze(1), (num_edges, self.embedding_dim, 3))\n",
    "        \n",
    "        # shape [E, embedding_dim] -> [E, embedding_dim, 3]\n",
    "        invariant_vectorial_message = invariant_vectorial_message.unsqueeze(-1) * relative_distance\n",
    "        equivariant_vectorial_message = equivariant_vectorial_message.unsqueeze(-1) * vectorial_feat[source]\n",
    "        vectorial_message = invariant_vectorial_message + equivariant_vectorial_message\n",
    "        \n",
    "        target_index_lifted = torch.broadcast_to(target_index_lifted.unsqueeze(-1), (num_edges, self.embedding_dim, 3))\n",
    "        # shape [N, embedding_dim, 3]\n",
    "        vectorial_message = torch.zeros(num_nodes, self.embedding_dim, 3).scatter_add_(0, target_index_lifted, vectorial_message)\n",
    "\n",
    "        return vectorial_message, scalar_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MessageBlock(10, 20, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 7.2948e-02,  3.0525e-01,  2.0908e-01],\n",
       "          [ 2.1422e-01,  2.3073e-01,  2.6142e-01],\n",
       "          [ 4.3262e-01,  4.5471e-01,  2.8785e-01],\n",
       "          [ 3.4445e-04, -2.1261e-02, -3.0663e-02],\n",
       "          [-1.5536e-01,  4.6588e-01,  5.6555e-01],\n",
       "          [-1.5878e-01,  3.0535e-02,  1.3734e-01],\n",
       "          [-7.1694e-01,  2.6245e-01, -2.2045e-01],\n",
       "          [ 5.1232e-02, -7.1793e-01, -5.6427e-01],\n",
       "          [-5.5384e-01, -5.2732e-01, -4.4876e-01],\n",
       "          [-3.0370e-01, -3.7373e-01, -2.8933e-01]],\n",
       " \n",
       "         [[ 5.6659e-02, -2.8151e-01, -2.5984e-01],\n",
       "          [-8.8295e-02, -1.6611e-02, -1.0053e-01],\n",
       "          [ 6.3831e-01,  4.7819e-01,  1.1651e-01],\n",
       "          [ 3.2418e-03,  5.4691e-02,  7.3105e-03],\n",
       "          [-2.2864e-01, -7.8132e-02, -2.4717e-01],\n",
       "          [-3.7047e-01, -3.7912e-01, -2.5769e-01],\n",
       "          [-1.3032e-01, -2.9274e-02, -1.0830e-01],\n",
       "          [-1.4560e-01,  8.0079e-02, -4.3078e-02],\n",
       "          [-2.0663e-01, -2.2244e-01, -2.4056e-01],\n",
       "          [ 5.2094e-02, -8.4580e-02,  3.2211e-02]],\n",
       " \n",
       "         [[ 3.6688e-01, -3.5209e-03,  1.0296e-01],\n",
       "          [ 3.1659e-01,  1.1026e-01,  2.4899e-01],\n",
       "          [ 4.3523e-01,  6.8190e-01,  3.3016e-01],\n",
       "          [-2.6835e-01, -3.1229e-01, -5.1594e-02],\n",
       "          [ 5.9491e-01,  2.2586e-01,  9.2410e-01],\n",
       "          [-1.3262e-01, -2.0982e-01,  2.1714e-01],\n",
       "          [ 2.3341e-01, -3.8142e-01, -1.1831e-01],\n",
       "          [-4.7216e-01, -1.7836e-01, -3.8994e-01],\n",
       "          [-2.6880e-01, -3.0835e-01, -2.0282e-01],\n",
       "          [-3.3649e-01, -1.1994e-01, -2.7177e-01]],\n",
       " \n",
       "         [[ 2.8069e-01, -1.8473e-01, -2.8032e-01],\n",
       "          [ 2.1105e-01, -1.8759e-02, -4.9665e-02],\n",
       "          [ 1.8106e-01,  1.7609e-01,  1.2671e-01],\n",
       "          [-5.7077e-02, -1.0579e-01, -1.2601e-01],\n",
       "          [ 9.7624e-01,  3.5368e-01, -1.3513e-02],\n",
       "          [-1.0656e-01, -4.8255e-01, -5.5859e-01],\n",
       "          [-3.5686e-01, -1.1418e-01, -2.4982e-01],\n",
       "          [-4.2870e-01, -3.3686e-01, -4.6601e-01],\n",
       "          [-1.6561e-02, -6.2393e-02, -1.3846e-01],\n",
       "          [-5.3700e-02, -2.2908e-02, -5.2168e-03]],\n",
       " \n",
       "         [[ 2.1087e-01,  9.3842e-02,  1.1589e-01],\n",
       "          [-7.2534e-01, -3.6904e-02, -2.6898e-01],\n",
       "          [ 8.6322e-01,  6.0867e-01,  6.3407e-01],\n",
       "          [-1.9415e-01, -1.5204e-01, -7.9330e-02],\n",
       "          [ 1.6161e-01,  2.1703e-01, -1.5097e-02],\n",
       "          [-6.9908e-01, -4.4443e-01, -4.7547e-01],\n",
       "          [-1.2637e+00, -1.9180e-01, -7.2603e-01],\n",
       "          [ 1.9334e-01, -1.0946e-01, -1.7543e-02],\n",
       "          [-9.9584e-01, -2.6168e-01, -4.6995e-01],\n",
       "          [ 1.7902e-01, -1.8079e-01, -1.9477e-02]]],\n",
       "        grad_fn=<ScatterAddBackward0>),\n",
       " tensor([[ 0.5018, -0.7402, -0.4100, -0.8374,  0.5858, -0.9926,  0.7482,  0.0000,\n",
       "           0.0699,  0.0119],\n",
       "         [ 0.7119, -0.8535, -0.7698, -0.9890,  0.7267, -0.5941,  0.1009, -0.2613,\n",
       "           0.0951,  0.0103],\n",
       "         [ 0.4118, -0.6421, -0.3291, -1.0564,  0.7219, -1.2970,  0.0828, -0.2793,\n",
       "           0.1097,  0.0016],\n",
       "         [ 0.6058, -0.0179, -0.2469, -0.2868,  0.0769, -0.5730,  0.2965, -0.0534,\n",
       "           0.1743,  0.0547],\n",
       "         [ 0.4311, -0.7630, -0.2144, -1.2153,  0.4074, -1.0163,  0.4971, -0.2996,\n",
       "           0.0484,  0.2971]], grad_fn=<ScatterAddBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.forward(v_feat, node_feat, node_pos, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(UpdateBlock, self).__init__()\n",
    "\n",
    "        self.scalar_feat_proj = nn.Sequential(\n",
    "                nn.Linear(2 * embedding_dim, embedding_dim),\n",
    "                ScaledSiLU(),\n",
    "                nn.Linear(embedding_dim, 3 * embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.vectorial_feat_proj = nn.Linear(embedding_dim, 2 * embedding_dim, bias = False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.scalar_feat_proj[0].weight)\n",
    "        self.scalar_feat_proj[0].bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.scalar_feat_proj[2].weight)\n",
    "        self.scalar_feat_proj[2].bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.vectorial_feat_proj.weight)\n",
    "\n",
    "    def forward(self, vectorial_feat: Tensor, scalar_feat: Tensor):\n",
    "        num_nodes = vectorial_feat.shape[0]\n",
    "        # shape [N, embedding_dim, 3] -> [N, 2 * embedding_dim, 3]\n",
    "        vectorial_feat = self.vectorial_feat_proj(vectorial_feat.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # shape [N, 2 * embedding_dim, 3] -> [N, embedding_dim, 2, 3] -> split [N, embedding_dim, 1, 3] [N, embedding_dim, 1, 3] \n",
    "        U, V = torch.split(vectorial_feat.view(num_nodes, -1, 2, 3), [1, 1], dim = 2)\n",
    "        # shape [N, embedding_dim, 1, 3] -> [N, embedding_dim, 3]\n",
    "        U, V = U.squeeze(2), V.squeeze(2)\n",
    "\n",
    "        # shape [N, 2 * embedding_dim] -> [N, 3 * embedding_dim]\n",
    "        a = self.scalar_feat_proj(torch.cat([scalar_feat, torch.sum(V, dim = -1)], dim = -1))\n",
    "        \n",
    "        # shape [N, 3 * embedding_dim] -> [N, embedding_dim, 3] -> split into 3 tensors [N, embedding_dim]\n",
    "        a = a.view(num_nodes, -1, 3)\n",
    "        a_vv, a_sv, a_ss = torch.split(a, [1, 1, 1], dim = -1)\n",
    "        a_vv, a_sv, a_ss = a_vv.squeeze(-1), a_sv.squeeze(-1), a_ss.squeeze(-1)\n",
    "\n",
    "        # [N, embedding_dim]\n",
    "        scalar_product = torch.sum(U * V, dim = -1)\n",
    "        # [N, embedding_dim]\n",
    "        scalar_update = a_ss + a_sv * scalar_product\n",
    "\n",
    "        # [N, embedding_dim, 3]\n",
    "        vectorial_update = U * a_vv.unsqueeze(-1)\n",
    "\n",
    "        return vectorial_update, scalar_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = UpdateBlock(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaiNN(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_blocks: int, num_radial_basis: int, cut_off: float, out_dim: int):\n",
    "        super(PaiNN, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_block = EmbeddingBlock(embedding_dim)\n",
    "\n",
    "        self.message_blocks = nn.ModuleList([\n",
    "            MessageBlock(embedding_dim, num_radial_basis, cut_off) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.update_blocks = nn.ModuleList([\n",
    "            UpdateBlock(embedding_dim) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.out_proj = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            ScaledSiLU(),\n",
    "            nn.Linear(embedding_dim // 2, out_dim)\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.out_proj[0].weight)\n",
    "        self.out_proj[0].bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.out_proj[2].weight)\n",
    "        self.out_proj[2].bias.data.fill_(0)\n",
    "\n",
    "        for message_block, update_block in zip(self.message_blocks, self.update_blocks):\n",
    "            message_block.reset_parameters()\n",
    "            update_block.reset_parameters()\n",
    "\n",
    "    def forward(self, atomic_num: Tensor, node_pos: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Parameters:\n",
    "            atomic_num (torch.Tensor):\n",
    "                Atomic number of each atom in the molecular graph. Shape [N]\n",
    "            node_pos (torch.Tensor):\n",
    "                Atoms' 3D coordinates. Shape [N, 3]\n",
    "            edge_index (torch.Tensor):\n",
    "                Shape [2, E]\n",
    "        \"\"\"\n",
    "        num_nodes = atomic_number.shape[0]\n",
    "        # Initialize vectorial representations and scalar representations\n",
    "        # Shape [N, embedding_dim, 3], [N, embedding_dim]\n",
    "        vectorial_feat, scalar_feat = torch.zeros(num_nodes, self.embedding_dim, 3), self.embedding_block(atomic_num)\n",
    "        \n",
    "        _, scalar_message = self.message_blocks[0](vectorial_feat, scalar_feat, node_pos, edge_index)\n",
    "        scalar_feat = scalar_feat + scalar_message\n",
    "        vectorial_update, scalar_update = self.update_blocks[0](vectorial_feat, scalar_feat)\n",
    "\n",
    "        for message_block, update_block in zip(self.message_blocks[1:], self.update_blocks[1:]):\n",
    "            vectorial_message, scalar_message = message_block(vectorial_feat, scalar_feat, node_pos, edge_index)\n",
    "            vectorial_feat = vectorial_feat + vectorial_message\n",
    "            scalar_feat = scalar_feat + scalar_message\n",
    "\n",
    "            vectorial_update, scalar_update = update_block(vectorial_feat, scalar_feat)\n",
    "            vectorial_feat = vectorial_feat + vectorial_message\n",
    "            scalar_feat = scalar_feat + scalar_message\n",
    "\n",
    "        # [N, out_dim]\n",
    "        scalar = self.out_proj(scalar_feat)\n",
    "\n",
    "        return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PaiNN(10, 5, 20, 0.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.forward(atomic_number, node_pos, edge_index).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (v3.8.10:3d8993a744, May  3 2021, 09:09:08) \n[Clang 12.0.5 (clang-1205.0.22.9)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

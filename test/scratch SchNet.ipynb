{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schnet\n",
    "# https://arxiv.org/pdf/1706.08566.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction block\n",
    "# cfconv\n",
    "# ssp(x) = ln(0.5(exp(x + 1))) = ln(0.5) + ln(exp(x) + 1) = ln(exp(x) + 1) - log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape: \n",
    "# in_node_feat: [N, feat_dim]\n",
    "# node_pos: [N, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feat = torch.from_numpy(np.random.rand(5, 10)).to(torch.float32)\n",
    "node_pos = torch.from_numpy(np.random.rand(5, 3)).to(torch.float32)\n",
    "edge_index = np.array([[0, 1], [1, 0],\n",
    "                        [0, 2], [2, 0],\n",
    "                        [0, 3], [3, 0],\n",
    "                        [0, 4], [4, 0],\n",
    "                        [1, 2], [2, 1],\n",
    "                        [1, 4], [4, 1],\n",
    "                        [2, 3], [3, 2],\n",
    "                        [2, 4], [4, 2], \n",
    "                        [3, 4], [4, 3],]).T\n",
    "edge_index = torch.from_numpy(edge_index).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssp(x) = ln(0.5(exp(x + 1))) = ln(0.5) + ln(exp(x) + 1)\n",
    "# why using this activation function? \n",
    "def shifted_softplus(x, shift):\n",
    "    r\"\"\"\n",
    "    Parameters:\n",
    "        x: torch.tensor\n",
    "        shift: float\n",
    "\n",
    "    Returns:\n",
    "        ln(exp(x) + 1) + ln(0.5)\n",
    "    \"\"\"\n",
    "    return nn.functional.softplus(x) + torch.log(torch.tensor(0.5)).to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.linspace(0, 30, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [E, 300]\n",
    "a = torch.broadcast_to(mu.unsqueeze(0), (edge_index.shape[-1], 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [E\n",
    "d = torch.sum((node_pos[source] - node_pos[target]) ** 2, dim = -1) ** 0.5\n",
    "# [E] -> [E, 300]\n",
    "d = torch.broadcast_to(d.unsqueeze(-1), (edge_index.shape[-1], 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why fix gamma and split mu evenly?\n",
    "def radial_basis_function(x, gamma, mu):\n",
    "    r\"\"\"\n",
    "    Parameters:\n",
    "        x (torch.tensor):\n",
    "        gamma (float):\n",
    "        mu (torch.tensor)\n",
    "    \"\"\"\n",
    "    return torch.exp(-gamma * ((x - mu) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "class FilterGeneratingNetworks(nn.Module):\n",
    "    def __init__(self, num_filters: int):\n",
    "        super(FilterGeneratingNetworks, self).__init__()\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            num_filters (int):\n",
    "                number of filters\n",
    "        \"\"\"\n",
    "        self.num_filters = num_filters\n",
    "        self.rbf = radial_basis_function\n",
    "\n",
    "    def forward(self, node_pos: Tensor, edge_index: Tensor, lower_bound: float, upper_bound: float, gamma: float) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Parameters:\n",
    "            lower_bound (float):\n",
    "                lower bound for mu values\n",
    "            upper_bound (float):\n",
    "                upper bound for mu values\n",
    "            gamma (float)\n",
    "            node_pos (torch.tensor):\n",
    "                3D coordinates of nodes. Shape [N, 3]\n",
    "            edge_index (torch.tensor)\n",
    "                Shape [2, E]\n",
    "\n",
    "        Returns:\n",
    "            expanded_distance (torch.tensor):\n",
    "                Shape [E, num_filters]\n",
    "        \"\"\"\n",
    "        source, target = edge_index\n",
    "        # shape [E]\n",
    "        distance = torch.sum((node_pos[source] - node_pos[target]) ** 2, dim = -1) ** 0.5\n",
    "        # shape [E] -> [E, num_filters]\n",
    "        distance_lifted = torch.broadcast_to(distance.unsqueeze(-1), (edge_index.shape[-1], self.num_filters))\n",
    "\n",
    "        # shape [num_filters]\n",
    "        mu = torch.linspace(lower_bound, upper_bound, self.num_filters)\n",
    "        # shape [num_filters] -> [E, num_filters]\n",
    "        mu_lifted = torch.broadcast_to(mu.unsqueeze(0), (edge_index.shape[-1], self.num_filters))\n",
    "\n",
    "        expanded_distance = self.rbf(distance_lifted, gamma, mu_lifted)\n",
    "\n",
    "        return expanded_distance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFConv(nn.Module):\n",
    "    def __init__(self, num_filters: int, hidden_dim: int, out_dim: int):\n",
    "        super(CFConv, self).__init__()\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            num_filters (int):\n",
    "                number of filters filter-generating networks\n",
    "            hidden_dim (int):\n",
    "                hidden dim for linear layer\n",
    "            out_dim (int):\n",
    "                final output dimension for CFConv\n",
    "        \"\"\"\n",
    "        self.num_filters = num_filters\n",
    "        self.out_dim = out_dim\n",
    "        self.filter_generating = FilterGeneratingNetworks(num_filters)\n",
    "        self.linear_1 = nn.Linear(num_filters, hidden_dim)\n",
    "        self.linear_2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.activation = shifted_softplus\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight)\n",
    "        self.linear_1.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight)\n",
    "        self.linear_2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, in_node_feat: Tensor, node_pos: Tensor, edge_index: Tensor, lower_bound: float, upper_bound: float, gamma: float, shift = None) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Apply Continuous-filter convolution on input node features\n",
    "\n",
    "        Parameters:\n",
    "            in_node_feat (torch.tensor):\n",
    "                Input node features. Shape [N, feat_dim]\n",
    "            node_pos (torch.tensor):\n",
    "                3D coordinates of nodes. Shape [N, 3]\n",
    "            edge_index (torch.tensor):\n",
    "                Shape [2, E]\n",
    "            upper_bound (float):\n",
    "                Upper bound for mu values\n",
    "            lower_bound (float):\n",
    "                Lower bound for mu values\n",
    "            gamma (float):\n",
    "                gamma value for Radial Basis Function\n",
    "            shift (float):\n",
    "                shift value for Shifted Softplus function. If None then shift is assigned to 0.5\n",
    "\n",
    "        Returns: \n",
    "            out_node_feat (torch.tensor):\n",
    "                Output node features. Shape [N, out_dim]\n",
    "        \"\"\"\n",
    "        assert in_node_feat.shape[-1] == self.out_dim\n",
    "        # shape [E, num_filters]\n",
    "        expanded_distance = self.filter_generating(node_pos, edge_index, lower_bound, upper_bound, gamma)\n",
    "        # shape [E, num_filters] -> [E, hidden_dim]\n",
    "        expanded_distance = self.linear_1(expanded_distance)\n",
    "        if shift is None:\n",
    "            shift = 0.5\n",
    "        expanded_distance = self.activation(expanded_distance, shift)\n",
    "        # shape [E, hidden_dim] -> [E, out_dim]\n",
    "        expanded_distance = self.linear_2(expanded_distance)\n",
    "        expanded_distance = self.activation(expanded_distance, shift)\n",
    "\n",
    "        assert expanded_distance.shape[-1] == in_node_feat.shape[-1]\n",
    "\n",
    "        source_index, target_index = edge_index\n",
    "        # compute message for each edge. shape [E, out_dim]\n",
    "        message = in_node_feat[target_index] * expanded_distance\n",
    "\n",
    "        # aggregrate message\n",
    "        # shape [N, out_dim]\n",
    "        out_node_feat = torch.zeros(in_node_feat.shape[0], self.out_dim).to(message.dtype)\n",
    "        # lift target index. shape [E] -> [E, out_dim]\n",
    "        target_index_lifted = torch.broadcast_to(target_index.unsqueeze(-1), (edge_index.shape[-1], self.out_dim))\n",
    "        out_node_feat = out_node_feat.scatter_add_(0, target_index_lifted, message)\n",
    "\n",
    "        return out_node_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_filters: int, hidden_dim: int):\n",
    "        super(InteractionBlock, self).__init__()\n",
    "        \n",
    "        self.atom_wise_layer_1 = nn.Linear(in_dim, in_dim)\n",
    "        self.cfconv = CFConv(num_filters, hidden_dim, in_dim)\n",
    "        self.atom_wise_layer_2 = nn.Linear(in_dim, in_dim)\n",
    "        self.atom_wise_layer_3 = nn.Linear(in_dim, in_dim)\n",
    "        self.activation = shifted_softplus\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.atom_wise_layer_1.weight)\n",
    "        self.atom_wise_layer_1.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.atom_wise_layer_2.weight)\n",
    "        self.atom_wise_layer_2.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.atom_wise_layer_3.weight)\n",
    "        self.atom_wise_layer_3.bias.data.fill_(0)\n",
    "        \n",
    "\n",
    "    def forward(self, in_node_feat: Tensor, node_pos: Tensor, edge_index: Tensor, lower_bound: Tensor, upper_bound: Tensor, gamma: Tensor, shift = None) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Parameters:\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        if shift is None:\n",
    "            shift = 0.5\n",
    "            \n",
    "        # [N, in_dim] -> [N, in_dim]\n",
    "        node_feat = self.atom_wise_layer_1(in_node_feat)\n",
    "\n",
    "        node_feat = self.cfconv(node_feat, node_pos, edge_index, lower_bound, upper_bound, gamma, shift)\n",
    "        node_feat = self.atom_wise_layer_2(node_feat)\n",
    "        node_feat = self.activation(node_feat, shift)\n",
    "        out_node_feat = self.atom_wise_layer_3(node_feat)\n",
    "        # residual connection\n",
    "        out_node_feat = out_node_feat + in_node_feat\n",
    "\n",
    "        return out_node_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = InteractionBlock(node_feat.shape[-1], 300, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.forward(node_feat, node_pos, edge_index, 0, 300, 10, 0.5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class ShiftedSoftplus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShiftedSoftplus, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softplus(x) - torch.log(torch.tensor(2)).to(x.dtype)\n",
    "\n",
    "class RadialBasisFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RadialBasisFunction, self).__init__()\n",
    "\n",
    "    def forward(self, x, gamma, mu):\n",
    "        r\"\"\"\n",
    "        Parameters:\n",
    "            x (torch.tensor):\n",
    "            gamma (float):\n",
    "            mu (torch.tensor)\n",
    "        \"\"\"\n",
    "        return torch.exp(-gamma * ((x - mu) ** 2))\n",
    "\n",
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "\n",
    "        self.atomic_num_embedding = nn.Embedding(95, embedding_dim, padding_idx = 0)\n",
    "\n",
    "    def forward(self, atomic_num: Tensor):\n",
    "        r\"\"\"\n",
    "        Initialize atom's representation\n",
    "\n",
    "        Parameters:\n",
    "            atomic_num (torch.Tensor):\n",
    "                Shape [N]\n",
    "\n",
    "        Returns:\n",
    "            node_feat (torch.Tensor):\n",
    "                Shape [N, embedding_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.atomic_num_embedding(atomic_num)\n",
    "\n",
    "class FilterGeneratingNetworks(nn.Module):\n",
    "    def __init__(self, num_filters):\n",
    "        super(FilterGeneratingNetworks, self).__init__()\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            num_filters (int):\n",
    "                number of filters\n",
    "        \"\"\"\n",
    "        self.num_filters = num_filters\n",
    "        self.rbf = RadialBasisFunction()\n",
    "\n",
    "    def forward(self, node_pos, edge_index, lower_bound, upper_bound, gamma):\n",
    "        r\"\"\"\n",
    "        Parameters:\n",
    "            lower_bound (float):\n",
    "                lower bound for mu values\n",
    "            upper_bound (float):\n",
    "                upper bound for mu values\n",
    "            gamma (float)\n",
    "            node_pos (torch.tensor):\n",
    "                3D coordinates of nodes. Shape [N, 3]\n",
    "            edge_index (torch.tensor)\n",
    "                Shape [2, E]\n",
    "\n",
    "        Returns:\n",
    "            expanded_distance (torch.tensor):\n",
    "                Shape [E, num_filters]\n",
    "        \"\"\"\n",
    "        source, target = edge_index\n",
    "        # shape [E]\n",
    "        distance = torch.sum((node_pos[source] - node_pos[target]) ** 2, dim = -1) ** 0.5\n",
    "        # shape [E] -> [E, num_filters]\n",
    "        distance_lifted = torch.broadcast_to(distance.unsqueeze(-1), (edge_index.shape[-1], self.num_filters))\n",
    "\n",
    "        # shape [num_filters]\n",
    "        mu = torch.linspace(lower_bound, upper_bound, self.num_filters)\n",
    "        # shape [num_filters] -> [E, num_filters]\n",
    "        mu_lifted = torch.broadcast_to(mu.unsqueeze(0), (edge_index.shape[-1], self.num_filters))\n",
    "\n",
    "        expanded_distance = self.rbf(distance_lifted, gamma, mu_lifted)\n",
    "\n",
    "        return expanded_distance    \n",
    "    \n",
    "# Why Continuous\n",
    "# The discrete filter (left) is not able to capture the subtle positional changes of the atoms resulting in \n",
    "# discontinuous energy predictions Eˆ (bottom left). The continuous filter captures these changes and yields \n",
    "# smooth energy predictions (bottom right).  \n",
    "\n",
    "class CFConv(nn.Module):\n",
    "    def __init__(self, num_filters, hidden_dim, out_dim):\n",
    "        super(CFConv, self).__init__()\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            num_filters (int):\n",
    "                number of filters filter-generating networks\n",
    "            hidden_dim (int):\n",
    "                hidden dim for linear layer\n",
    "            out_dim (int):\n",
    "                final output dimension for CFConv\n",
    "        \"\"\"\n",
    "        self.num_filters = num_filters\n",
    "        self.out_dim = out_dim\n",
    "        self.filter_generating = FilterGeneratingNetworks(num_filters)\n",
    "        self.linear_1 = nn.Linear(num_filters, hidden_dim)\n",
    "        self.linear_2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.activation = ShiftedSoftplus()\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.linear_1.weight)\n",
    "        self.linear_1.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.linear_2.weight)\n",
    "        self.linear_2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, in_node_feat, node_pos, edge_index, lower_bound, upper_bound, gamma):\n",
    "        r\"\"\"\n",
    "        Apply Continuous-filter convolution on input node features\n",
    "\n",
    "        Parameters:\n",
    "            in_node_feat (torch.tensor):\n",
    "                Input node features. Shape [N, feat_dim]\n",
    "            node_pos (torch.tensor):\n",
    "                3D coordinates of nodes. Shape [N, 3]\n",
    "            edge_index (torch.tensor):\n",
    "                Shape [2, E]\n",
    "            upper_bound (float):\n",
    "                Upper bound for mu values\n",
    "            lower_bound (float):\n",
    "                Lower bound for mu values\n",
    "            gamma (float):\n",
    "                gamma value for Radial Basis Function\n",
    "\n",
    "        Returns: \n",
    "            out_node_feat (torch.tensor):\n",
    "                Output node features. Shape [N, out_dim]\n",
    "        \"\"\"\n",
    "        assert in_node_feat.shape[-1] == self.out_dim\n",
    "        # shape [E, num_filters]\n",
    "        expanded_distance = self.filter_generating(node_pos, edge_index, lower_bound, upper_bound, gamma)\n",
    "        # shape [E, num_filters] -> [E, hidden_dim]\n",
    "        expanded_distance = self.linear_1(expanded_distance)\n",
    "        \n",
    "        expanded_distance = self.activation(expanded_distance)\n",
    "        # shape [E, hidden_dim] -> [E, out_dim]\n",
    "        expanded_distance = self.linear_2(expanded_distance)\n",
    "        expanded_distance = self.activation(expanded_distance)\n",
    "\n",
    "        assert expanded_distance.shape[-1] == in_node_feat.shape[-1]\n",
    "\n",
    "        source_index, target_index = edge_index\n",
    "        # compute message for each edge. shape [E, out_dim]\n",
    "        message = in_node_feat[target_index] * expanded_distance\n",
    "\n",
    "        # aggregrate message\n",
    "        # shape [N, out_dim]\n",
    "        out_node_feat = torch.zeros(in_node_feat.shape[0], self.out_dim).to(message.dtype)\n",
    "        # lift target index. shape [E] -> [E, out_dim]\n",
    "        target_index_lifted = torch.broadcast_to(target_index.unsqueeze(-1), (edge_index.shape[-1], self.out_dim))\n",
    "        out_node_feat = out_node_feat.scatter_add_(0, target_index_lifted, message)\n",
    "\n",
    "        return out_node_feat\n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, in_dim, num_filters, hidden_dim):\n",
    "        super(InteractionBlock, self).__init__()\n",
    "        \n",
    "        self.atom_wise_layer_1 = nn.Linear(in_dim, in_dim)\n",
    "        self.cfconv = CFConv(num_filters, hidden_dim, in_dim)\n",
    "        self.atom_wise_layer_2 = nn.Linear(in_dim, in_dim)\n",
    "        self.atom_wise_layer_3 = nn.Linear(in_dim, in_dim)\n",
    "        self.activation = ShiftedSoftplus()\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.atom_wise_layer_1.weight)\n",
    "        self.atom_wise_layer_1.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.atom_wise_layer_2.weight)\n",
    "        self.atom_wise_layer_2.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.atom_wise_layer_3.weight)\n",
    "        self.atom_wise_layer_3.bias.data.fill_(0)\n",
    "        \n",
    "\n",
    "    def forward(self, in_node_feat, node_pos, edge_index, lower_bound, upper_bound, gamma):\n",
    "        r\"\"\"\n",
    "        Parameters:\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "            \n",
    "        # [N, in_dim] -> [N, in_dim]\n",
    "        node_feat = self.atom_wise_layer_1(in_node_feat)\n",
    "\n",
    "        node_feat = self.cfconv(node_feat, node_pos, edge_index, lower_bound, upper_bound, gamma)\n",
    "        node_feat = self.atom_wise_layer_2(node_feat)\n",
    "        node_feat = self.activation(node_feat)\n",
    "        out_node_feat = self.atom_wise_layer_3(node_feat)\n",
    "        # residual connection\n",
    "        out_node_feat = out_node_feat + in_node_feat\n",
    "\n",
    "        return out_node_feat\n",
    "\n",
    "class SchNet(nn.Module):\n",
    "    def __init__(self, num_interaction_block: int, hidden_dim: int, num_filters):\n",
    "        super(SchNet, self).__init__()\n",
    "\n",
    "        self.embedding_block = EmbeddingBlock(hidden_dim)\n",
    "\n",
    "        self.interaction_blocks = nn.ModuleList([\n",
    "            InteractionBlock(hidden_dim, num_filters, hidden_dim) for _ in range(num_interaction_block)\n",
    "        ])\n",
    "\n",
    "        self.linear_1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.activation = ShiftedSoftplus()\n",
    "        self.linear_2 = nn.Linear(hidden_dim // 2, 1)\n",
    "\n",
    "    def forward(self, atomic_num: Tensor, node_pos: Tensor, edge_index: Tensor,\n",
    "            lower_bound: float = 0.0, upper_bound: float = 30.0, gamma: float = 10.0):\n",
    "        r\"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Shape [N, hidden_dim]\n",
    "        in_node_feat = self.embedding_block(atomic_num)\n",
    "\n",
    "        for interaction_block in self.interaction_blocks:\n",
    "            in_node_feat = interaction_block(in_node_feat, node_pos, edge_index, lower_bound, upper_bound, gamma)\n",
    "\n",
    "        # Shape [N, hidden_dim] -> [N, hidden_dim // 2]\n",
    "        in_node_feat = self.linear_1(in_node_feat)\n",
    "        in_node_feat = self.activation(in_node_feat)\n",
    "\n",
    "        # Shape [N, hidden_dim] -> [N, 1]\n",
    "        in_node_feat = self.linear_2(in_node_feat)\n",
    "\n",
    "        energy = torch.sum(in_node_feat.squeeze())\n",
    "\n",
    "        return energy\n",
    "\n",
    "node_feat = torch.from_numpy(np.random.rand(5, 10)).to(torch.float32)\n",
    "node_pos = torch.from_numpy(np.random.rand(5, 3)).to(torch.float32)\n",
    "edge_index = np.array([[0, 1], [1, 0],\n",
    "                        [0, 2], [2, 0],\n",
    "                        [0, 3], [3, 0],\n",
    "                        [0, 4], [4, 0],\n",
    "                        [1, 2], [2, 1],\n",
    "                        [1, 4], [4, 1],\n",
    "                        [2, 3], [3, 2],\n",
    "                        [2, 4], [4, 2], \n",
    "                        [3, 4], [4, 3],]).T\n",
    "edge_index = torch.from_numpy(edge_index).to(torch.long)\n",
    "\n",
    "atomic_num = torch.randint(0, 95, (5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SchNet(10, 15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.2510, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.forward(atomic_num, node_pos, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
